{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45bdbb6",
   "metadata": {},
   "source": [
    "# ISM 6404: BI and Data Visualization\n",
    "# Special Types of Data\n",
    "## Prof. Leandro Nunes de Castro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8168af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This content was created as a supporting material for the course\n",
    "# ISM 6404 - BI and Data Visualization\n",
    "# Prof. Leandro de Castro (c), Spring 2024\n",
    "# All rights reserved\n",
    "\n",
    "# Florida Gulf Coast University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efb593",
   "metadata": {},
   "source": [
    "# 1 Time Series Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset1\n",
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
    "import pandas as pd\n",
    "\n",
    "dclimate = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "dclimate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "dclimate.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dclimate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate the line charts, boxplots, scatterplot matrix and heatmap\n",
    "# of the Daily Delhi Climate Train Data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Line charts for the Daily Delhi Climate Train Data\n",
    "df = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "dDelhi = df[['meantemp', 'humidity', 'wind_speed', 'meanpressure', 'date']]\n",
    "fig, axes = plt.subplots(nrows=4, figsize=(10, 15))\n",
    "vars_and_indices = [('meantemp', 0), ('humidity', 1), ('wind_speed', 2), ('meanpressure', 3)]\n",
    "for var, i in vars_and_indices:\n",
    "    axes[i].plot(dDelhi['date'], dDelhi[var], linestyle='-')\n",
    "    axes[i].set_title(var.replace('-', ' ').title())\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel(var.replace('-', ' ').title())\n",
    "    axes[i].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots for the Daily Delhi Climate Train Data\n",
    "dDelhi = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "variables = ['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
    "dDelhi_normalized = (dDelhi[variables] - dDelhi[variables].min()) / (dDelhi[variables].max() - dDelhi[variables].min())\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].boxplot(dDelhi[variables].values)\n",
    "axes[0].set_title('Boxplot - Original Data')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_xticklabels(variables, rotation=45)\n",
    "axes[1].boxplot(dDelhi_normalized.values)\n",
    "axes[1].set_title('Boxplot - Normalized Data')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_xticklabels(variables, rotation=45)\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot matrix and Heatmap for the Daily Delhi Climate Train Data\n",
    "dDelhi = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "variables = ['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
    "sns.set(style='ticks')\n",
    "sns.pairplot(dDelhi[variables])\n",
    "plt.suptitle('Scatterplot Matrix', y=1.02)\n",
    "plt.show()\n",
    "corr_matrix = dDelhi[variables].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap - Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6638b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving averages for variables 'meantemp' and 'humidity' \n",
    "# of the Daily Delhi Climate Train Data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset and extract the variables\n",
    "df = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "dDelhi = df[['meantemp', 'humidity', 'date']]\n",
    "\n",
    "# Create a single figure with subplots stacked vertically\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 8))\n",
    "vars_and_indices = [('meantemp', 0), ('humidity', 1)]\n",
    "for var, i in vars_and_indices:\n",
    "    axes[i].plot(dDelhi['date'], dDelhi[var], linestyle='-', color='gray', \n",
    "                 alpha=.5, label='Original')\n",
    "    axes[i].plot(dDelhi['date'], dDelhi[var].rolling(window=20, min_periods=1).mean(), \n",
    "                 linestyle='--', label='Moving Average')\n",
    "    axes[i].set_title(var.replace('-', ' ').title(), fontsize=14) \n",
    "    axes[i].set_xlabel('Date', fontsize=12) \n",
    "    axes[i].set_ylabel(var.replace('-', ' ').title(), fontsize=12)\n",
    "    axes[i].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    axes[i].legend(fontsize=12) \n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55265669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time series decomposition of the 'meantemp' variable \n",
    "# of the Daily Delhi Climate Train data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Load the dataset\n",
    "dDelhi = pd.read_csv('DailyDelhiClimateTrain.csv')\n",
    "\n",
    "# Extract the variable and set the date column as the index\n",
    "variable = 'meantemp'\n",
    "dDelhi['date'] = pd.to_datetime(dDelhi['date'])\n",
    "dDelhi.set_index('date', inplace=True)\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(dDelhi[variable], model='additive', period = 50)\n",
    "\n",
    "# Plot the original, trend, seasonal, and residual components\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "dDelhi[variable].plot(ax=axes[0])\n",
    "axes[0].set_ylabel('Original')\n",
    "decomposition.trend.plot(ax=axes[1])\n",
    "axes[1].set_ylabel('Trend')\n",
    "decomposition.seasonal.plot(ax=axes[2])\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "decomposition.resid.plot(ax=axes[3])\n",
    "axes[3].set_ylabel('Residual')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.suptitle('Time Series Decomposition - {}'.format(variable), fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947f132",
   "metadata": {},
   "source": [
    "## 6.2 Text and Document Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3407274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Download the IMDb dataset\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Calculate the dimensions\n",
    "num_documents = len(documents)\n",
    "num_categories = len(set(category for _, category in documents))\n",
    "\n",
    "# Print the dimensions\n",
    "print(\"Number of Documents (Reviews):\", num_documents)\n",
    "print(\"Number of Categories (Labels):\", num_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Download the IMDb dataset\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Print the first 10 objects in the dataset\n",
    "for i, (words, category) in enumerate(documents[:10], 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(\" \".join(words[:80]), \"...\")\n",
    "    print(\"Category:\", category)\n",
    "    print()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e596a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to structure texts using Lexical Representations\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download the IMDb corpus and the stoplist\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_docs = [\" \".join(words) for words, category in documents]\n",
    "\n",
    "# Stopwords removal and printing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stopwords contained in the stopwords file in NLTK:\")\n",
    "print(stop_words)\n",
    "filtered_docs = [\" \".join([word for word in word_tokenize(doc.lower()) \n",
    "                           if word.isalpha() and word not in stop_words])\n",
    "                 for doc in tokenized_docs]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_docs = [\" \".join([stemmer.stem(word) for word in word_tokenize(doc)])\n",
    "                for doc in filtered_docs]\n",
    "\n",
    "# Create the data matrix using different methods\n",
    "# Binary\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "data_matrix_binary = binary_vectorizer.fit_transform(stemmed_docs)\n",
    "\n",
    "# Absolute Frequency\n",
    "count_vectorizer = CountVectorizer()\n",
    "data_matrix_abs_freq = count_vectorizer.fit_transform(stemmed_docs)\n",
    "\n",
    "# Relative Frequency (Term-Frequency)\n",
    "tf_vectorizer = CountVectorizer()\n",
    "data_matrix_rel_freq = tf_vectorizer.fit_transform(stemmed_docs)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "data_matrix_tfidf = tfidf_vectorizer.fit_transform(stemmed_docs)\n",
    "\n",
    "# Printing the feature names (words) \n",
    "print(\"\\nFeature names (words) for the Data Matrix:\")\n",
    "print(binary_vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "# Printing each data matrix\n",
    "print(\"\\nBinary Data Matrix:\")\n",
    "print(data_matrix_binary[:5, :20].toarray())\n",
    "print(\"\\nAbsolute Frequency Data Matrix:\")\n",
    "print(data_matrix_abs_freq[:5, :20].toarray())\n",
    "print(\"\\nRelative Frequency (Term-Frequency) Data Matrix:\")\n",
    "print(data_matrix_rel_freq[:5, :20].toarray())\n",
    "print(\"\\nTF-IDF Data Matrix:\")\n",
    "print(data_matrix_tfidf[:5, :20].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate simple descriptive statistics for text data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Download the IMDb dataset and stopwords corpus\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Initialize variables for descriptive statistics\n",
    "word_count = 0\n",
    "unique_words = set()\n",
    "word_length_sum = 0\n",
    "sentence_count = 0\n",
    "sentence_length_sum = 0\n",
    "stopwords_count = 0\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for document, _ in documents:\n",
    "    # Tokenization and lowercase\n",
    "    tokens = word_tokenize(document.lower())\n",
    "    # Remove punctuation and digits\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    # Update word count and unique words\n",
    "    word_count += len(tokens)\n",
    "    unique_words.update(tokens)\n",
    "    # Update word length sum\n",
    "    word_length_sum += sum(len(word) for word in tokens)\n",
    "    # Update word frequencies\n",
    "    word_frequencies.update(tokens)\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(document)\n",
    "    # Update sentence count and sentence length sum\n",
    "    sentence_count += len(sentences)\n",
    "    sentence_length_sum += sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    # Count stopwords\n",
    "    stopwords_count += sum(1 for token in tokens if token in stopwords.words('english'))\n",
    "\n",
    "# Calculate descriptive statistics and print the results\n",
    "print(\"Descriptive Statistics for the IMDb Dataset:\")\n",
    "print(\"Word Count:\", word_count)\n",
    "print(\"Unique Word Count:\", len(unique_words))\n",
    "print(\"Vocabulary Size:\", len(unique_words))\n",
    "print(\"Average Word Length:\", word_length_sum / word_count)\n",
    "print(\"Most Common Words:\", word_frequencies.most_common(10))\n",
    "print(\"Sentence Count:\", sentence_count)\n",
    "print(\"Average Sentence Length:\", sentence_length_sum / sentence_count)\n",
    "print(\"Number of Stopwords:\", stopwords_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f972b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate specific descriptive statistics for text data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Download the IMDb dataset and stopwords corpus\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Part-of-Speech (POS) Distribution\n",
    "pos_tags = [tag for words, _ in documents for word, tag in nltk.pos_tag(word_tokenize(words))]\n",
    "pos_distribution = Counter(pos_tags)\n",
    "\n",
    "# Flesch-Kincaid Grade Level and Automated Readability Index (ARI)\n",
    "all_reviews_text = \" \".join([text for text, _ in documents])\n",
    "flesch_kincaid_grade = textstat.flesch_kincaid_grade(all_reviews_text)\n",
    "automated_readability_index = textstat.automated_readability_index(all_reviews_text)\n",
    "\n",
    "# Co-occurrence Matrix\n",
    "preprocessed_documents = [\" \".join([word.lower() for word in word_tokenize(words)])\n",
    "                          for words, _ in documents]\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(preprocessed_documents)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "\n",
    "# Print the results\n",
    "print(\"Part-of-Speech Distribution:\")\n",
    "print(pos_distribution)\n",
    "print(\"\\nFlesch-Kincaid Grade Level:\", flesch_kincaid_grade)\n",
    "print(\"\\nAutomated Readability Index:\", automated_readability_index)\n",
    "print(\"\\nCo-occurrence Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e47b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to generate a Tag Cloud and a Frequency Distribution \n",
    "# of the words in the IMDb corpus\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the IMDb dataset and stopwords corpus\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Concatenate all the reviews into a single text\n",
    "all_reviews_text = \" \".join([text for text, _ in documents])\n",
    "\n",
    "# Tokenization and Preprocessing\n",
    "tokens = word_tokenize(all_reviews_text.lower())\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "# Calculate word frequency\n",
    "word_frequency = FreqDist(filtered_tokens)\n",
    "\n",
    "# Generate the tag cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequency)\n",
    "\n",
    "# Display the tag cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Tag Cloud for IMDb Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Sort the word_frequency dictionary by frequency in descending order\n",
    "sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Get the first words and their frequencies\n",
    "top_25_words = list(sorted_word_frequency.keys())[:25]\n",
    "top_25_frequencies = list(sorted_word_frequency.values())[:25]\n",
    "\n",
    "# Plot the bar chart for the first 20 words\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_25_words, top_25_frequencies)\n",
    "plt.title('Top 25 Most Frequent Words in IMDb Dataset')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate the bi-gram and the tri-gram \n",
    "# for the IMDb corpus in NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the IMDb dataset and stopwords corpus\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Select only 10% of the documents\n",
    "num_documents = int(0.01 * len(documents))\n",
    "documents = documents[:num_documents]\n",
    "\n",
    "# Concatenate all the reviews into a single text\n",
    "all_reviews_text = \" \".join([text for text, _ in documents])\n",
    "\n",
    "# Tokenization and Preprocessing\n",
    "tokens = word_tokenize(all_reviews_text.lower())\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "# Generate 2-grams\n",
    "bi_grams = list(ngrams(filtered_tokens, 2))\n",
    "bi_gram_freq_dist = nltk.FreqDist([\" \".join(gram) for gram in bi_grams])\n",
    "\n",
    "# Generate 3-grams\n",
    "tri_grams = list(ngrams(filtered_tokens, 3))\n",
    "tri_gram_freq_dist = nltk.FreqDist([\" \".join(gram) for gram in tri_grams])\n",
    "\n",
    "# Generate the tag cloud for 2-grams\n",
    "bi_gram_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bi_gram_freq_dist)\n",
    "\n",
    "# Generate the tag cloud for 3-grams\n",
    "tri_gram_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(tri_gram_freq_dist)\n",
    "\n",
    "# Display the tag clouds\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(bi_gram_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('2-Gram Tag Cloud for IMDb Dataset')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(tri_gram_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('3-Gram Tag Cloud for IMDb Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e606f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate the heatmap for POS Tags\n",
    "# for the IMDb corpus \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the IMDb movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Tokenization and Preprocessing\n",
    "def preprocess_document(document):\n",
    "    tokens = nltk.word_tokenize(document.lower())\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Perform POS tagging and calculate word frequency in each tag category\n",
    "def calculate_word_frequency_with_pos(documents):\n",
    "    word_frequency = {}\n",
    "    all_tags = set()\n",
    "    for document in documents:\n",
    "        tokens = preprocess_document(document[0])\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        for word, pos_tag in tagged_tokens:\n",
    "            if word not in word_frequency:\n",
    "                word_frequency[word] = {}\n",
    "            if pos_tag not in word_frequency[word]:\n",
    "                word_frequency[word][pos_tag] = 0\n",
    "            word_frequency[word][pos_tag] += 1\n",
    "            all_tags.add(pos_tag)\n",
    "    \n",
    "    # Sort words by frequency and select the top 20\n",
    "    sorted_words = sorted(word_frequency.items(), key=lambda item: sum(item[1].values()), reverse=True)\n",
    "    top_20_words = [word for word, _ in sorted_words[:20]]\n",
    "    \n",
    "    # Create the matrix\n",
    "    all_words = list(top_20_words)\n",
    "    matrix = []\n",
    "    for word in all_words:\n",
    "        row = [word_frequency[word].get(tag, 0) for tag in all_tags]\n",
    "        matrix.append(row)\n",
    "    return matrix, list(all_words), list(all_tags)\n",
    "\n",
    "# Calculate word frequency with POS tags\n",
    "matrix, words, tag_categories = calculate_word_frequency_with_pos(documents)\n",
    "\n",
    "# Convert the matrix to a DataFrame for Seaborn heatmap\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(matrix, index=words, columns=tag_categories)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(df, cmap='YlGnBu', annot=False, fmt='d', cbar=True)\n",
    "plt.title('Text Heatmap for POS Tags (IMDb Corpus)')\n",
    "plt.xlabel('POS Tags')\n",
    "plt.ylabel('Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff476b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate the Dependency Parse Tree for the IMDb \n",
    "# dataset in NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Download the IMDb dataset\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Select the documents to be displayed\n",
    "documents = documents[:1]\n",
    "\n",
    "# Concatenate all the reviews into a single text\n",
    "all_reviews_text = \" \".join([text for text, _ in documents])\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(all_reviews_text)\n",
    "\n",
    "# Get individual sentences\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Print each sentence and its dependency tree\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\", sentence)\n",
    "    displacy.render(sentence, style='dep', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846260",
   "metadata": {},
   "source": [
    "## 6.3 Trees and Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "G.add_node(\"v1\")\n",
    "G.add_node(\"v2\")\n",
    "G.add_node(\"v3\")\n",
    "G.add_node(\"v4\")\n",
    "\n",
    "# Add edges\n",
    "G.add_edge(\"v1\", \"v2\")\n",
    "G.add_edge(\"v2\", \"v3\")\n",
    "G.add_edge(\"v3\", \"v4\")\n",
    "G.add_edge(\"v4\", \"v1\")\n",
    "\n",
    "# Plot the graph\n",
    "pos = nx.spring_layout(G)  # Positioning algorithm\n",
    "nx.draw(G, pos, with_labels=True, node_size=1000, node_color='skyblue', font_size=10, font_color='black', font_weight='bold', edge_color='gray')\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Simple Graph (Network) Example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eab73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (cities)\n",
    "cities = [\"Miami\", \"Orlando\", \"Tampa\", \"Jacksonville\", \"Tallahassee\", \"Fort Lauderdale\", \"Fort Myers\"]\n",
    "\n",
    "G.add_nodes_from(cities)\n",
    "\n",
    "# Add edges (connections between cities) with distances\n",
    "edges = [\n",
    "    (\"Miami\", \"Orlando\", {\"distance\": 230}),\n",
    "    (\"Miami\", \"Tampa\", {\"distance\": 280}),\n",
    "    (\"Miami\", \"Fort Lauderdale\", {\"distance\": 30}),\n",
    "    (\"Orlando\", \"Tampa\", {\"distance\": 85}),\n",
    "    (\"Tampa\", \"Jacksonville\", {\"distance\": 200}),\n",
    "    (\"Jacksonville\", \"Tallahassee\", {\"distance\": 160}),\n",
    "    (\"Tallahassee\", \"Orlando\", {\"distance\": 270}),\n",
    "    (\"Tallahassee\", \"Tampa\", {\"distance\": 230}),\n",
    "    (\"Fort Myers\", \"Tampa\", {\"distance\": 140}),\n",
    "    (\"Fort Myers\", \"Miami\", {\"distance\": 150}),  # Distance between Fort Myers and Miami\n",
    "]\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Plot the network with distances as edge labels\n",
    "pos = nx.spring_layout(G)  # Positioning algorithm\n",
    "nx.draw(G, pos, with_labels=True, node_size=1000, node_color='lightblue', font_size=10, font_weight='bold', edge_color='gray', arrowsize=20)\n",
    "\n",
    "# Add edge labels (distances)\n",
    "edge_labels = {(u, v): f\"{attr['distance']} mi\" for u, v, attr in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Network with Some Florida Cities Including Distances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c6094",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Script to generate a directed and weighted budget tree\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes (expense categories)\n",
    "nodes = [\"Gross Salary\", \"Expenses\",\"House\",\"Learn\",\"Leisure\",\"Market\",\"Utils\",\n",
    "    \"College\",\"Others\",\"Power\",\"W&W\",\"Rental\",\"Maint\",\"Savings\",\"Invest\",\"Retire\"]\n",
    "\n",
    "# Add edges (expense relationships) with values\n",
    "edges = [\n",
    "    (\"Gross Salary\", \"Expenses\", {'value': 5000}),\n",
    "    (\"Gross Salary\", \"Savings\", {'value': 1000}),\n",
    "    (\"Expenses\", \"House\", {'value': 1500}),\n",
    "    (\"Expenses\", \"Learn\", {'value': 1600}),\n",
    "    (\"Expenses\", \"Leisure\", {'value': 600}),\n",
    "    (\"Expenses\", \"Market\", {'value': 900}),\n",
    "    (\"Expenses\", \"Utils\", {'value': 400}),\n",
    "    (\"Learn\", \"College\", {'value': 1200}),\n",
    "    (\"Learn\", \"Others\", {'value': 400}),\n",
    "    (\"Utils\", \"Power\", {'value': 250}),\n",
    "    (\"Utils\", \"W&W\", {'value': 150}),\n",
    "    (\"House\", \"Rental\", {'value': 1200}),\n",
    "    (\"House\", \"Maint\", {'value': 300}),\n",
    "    (\"Savings\", \"Invest\", {'value': 400}),\n",
    "    (\"Savings\", \"Retire\", {'value': 600}),\n",
    "]\n",
    "\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Manually set node positions\n",
    "node_positions = {\n",
    "    \"Gross Salary\": (11, 12), \"Expenses\": (7, 8),\n",
    "    \"Savings\": (19, 8), \"House\": (1, 4),\n",
    "    \"Learn\": (7, 4), \"Leisure\": (10, 4),\n",
    "    \"Market\": (13, 4), \"Utils\": (4, 4),\n",
    "    \"College\": (8, 0), \"Others\": (12, 0),\n",
    "    \"Power\": (2, 0), \"W&W\": (5, 0),\n",
    "    \"Rental\": (-4, 0), \"Maint\": (-1, 0),\n",
    "    \"Invest\": (17, 4), \"Retire\": (21, 4),\n",
    "}\n",
    "\n",
    "# Get edge labels from edge attributes\n",
    "edge_labels = {(u, v): f\"${attr['value']}\" for u, v, attr in G.edges(data=True)}\n",
    "\n",
    "# Plot the expense tree with manual positions and edge labels\n",
    "nx.draw(G, pos=node_positions, with_labels=True, node_size=1000, node_color='lightblue', \n",
    "        font_size=10, font_weight='bold', edge_color='gray', arrowsize=20)\n",
    "nx.draw_networkx_edge_labels(G, pos=node_positions, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Budget Tree Example with Edge Values\")\n",
    "#plt.title(\"Budget Tree Example\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes and edges that have to be loaded before running the next code\n",
    "\n",
    "# Add nodes (expense categories)\n",
    "nodes = [\"Gross Salary\", \"Expenses\",\"House\",\"Learn\",\"Leisure\",\"Market\",\"Utils\",\n",
    "    \"College\",\"Others\",\"Power\",\"W&W\",\"Rental\",\"Maint\",\"Savings\",\"Invest\",\"Retire\"]\n",
    "\n",
    "# Add edges (expense relationships) with values\n",
    "edges = [\n",
    "    (\"Gross Salary\", \"Expenses\", {'value': 5000}),\n",
    "    (\"Gross Salary\", \"Savings\", {'value': 1000}),\n",
    "    (\"Expenses\", \"House\", {'value': 1500}),\n",
    "    (\"Expenses\", \"Learn\", {'value': 1600}),\n",
    "    (\"Expenses\", \"Leisure\", {'value': 600}),\n",
    "    (\"Expenses\", \"Market\", {'value': 900}),\n",
    "    (\"Expenses\", \"Utils\", {'value': 400}),\n",
    "    (\"Learn\", \"College\", {'value': 1200}),\n",
    "    (\"Learn\", \"Others\", {'value': 400}),\n",
    "    (\"Utils\", \"Power\", {'value': 250}),\n",
    "    (\"Utils\", \"W&W\", {'value': 150}),\n",
    "    (\"House\", \"Rental\", {'value': 1200}),\n",
    "    (\"House\", \"Maint\", {'value': 300}),\n",
    "    (\"Savings\", \"Invest\", {'value': 400}),\n",
    "    (\"Savings\", \"Retire\", {'value': 600}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54924c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the Descriptive Analysis of trees\n",
    "# It requires that the nodes and edges are loaded previously\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Calculate and print measures\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "\n",
    "# Calculate height of the tree (maximum depth from root)\n",
    "height = nx.dag_longest_path_length(G)\n",
    "print(\"Height of the tree:\", height)\n",
    "\n",
    "# Calculate degree of each node\n",
    "in_degree_dict = dict(G.in_degree())\n",
    "out_degree_dict = dict(G.out_degree())\n",
    "print(\"\\nIn-Degree of each node:\")\n",
    "for node in nodes:\n",
    "    print(f\"{node}: {in_degree_dict.get(node, 0)}\")\n",
    "print(\"\\nOut-Degree of each node:\")\n",
    "for node in nodes:\n",
    "    print(f\"{node}: {out_degree_dict.get(node, 0)}\")\n",
    "\n",
    "# Calculate branching factor (average out-degree)\n",
    "branching_factor = G.number_of_edges() / G.number_of_nodes()\n",
    "print(\"\\nBranching factor:\", branching_factor)\n",
    "\n",
    "# Calculate and print tree diameter\n",
    "diameter = 0\n",
    "for node in nodes:\n",
    "    if G.out_degree(node) == 0:  # Only consider leaf nodes\n",
    "        path_lengths = nx.single_source_shortest_path_length(G.reverse(), source=node)\n",
    "        max_path_length = max(path_lengths.values())\n",
    "        diameter = max(diameter, max_path_length)\n",
    "print(\"Tree Diameter:\", diameter)\n",
    "\n",
    "# Calculate level of each node (depth from the root)\n",
    "root = \"Gross Salary\"\n",
    "level_dict = {root: 0}\n",
    "for node in nodes:\n",
    "    if node != root:\n",
    "        parent = list(G.predecessors(node))[0]  # Assuming single parent\n",
    "        level_dict[node] = level_dict[parent] + 1\n",
    "print(\"\\nLevel of each node:\")\n",
    "for node in nodes:\n",
    "    print(f\"{node}: {level_dict[node]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60b32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to generate partial genealogic trees of Queen Elizabeth II\n",
    "# With internal functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the family members and relationships\n",
    "family_tree = {\n",
    "    \"Queen Elizabeth II\": [\"Prince Charles\", \"Princess Anne\", \"Prince Andrew\", \"Prince Edward\"],\n",
    "    \"Prince Philip\": [\"Prince Charles\", \"Princess Anne\", \"Prince Andrew\", \"Prince Edward\"],\n",
    "    \"Prince Charles\": [\"Prince William\", \"Prince Henry\"],\n",
    "    \"Princess Anne\": [],\n",
    "    \"Prince Andrew\": [],\n",
    "    \"Prince Edward\": [],\n",
    "    \"Prince William\": [\"Prince George\", \"Princess Charlotte\", \"Prince Louis\"],\n",
    "}\n",
    "\n",
    "def plot_genealogy(node, depth=0):\n",
    "    print(\"  \" * depth + \"|_\" + node)\n",
    "    if node in family_tree:\n",
    "        for child in family_tree[node]:\n",
    "            plot_genealogy(child, depth + 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Genealogical Tree:\")\n",
    "    plot_genealogy(\"Queen Elizabeth II\")\n",
    "    \n",
    "    # Plot using Matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    \n",
    "    def plot_tree(node, x, y, level=0):\n",
    "        ax.text(x, y, node, ha='center', va='center', \n",
    "                bbox=dict(facecolor='lightblue', edgecolor='gray', boxstyle='round,pad=0.3'))\n",
    "        if node in family_tree:\n",
    "            num_children = len(family_tree[node])\n",
    "            child_spacing = 18 / (num_children + 6)\n",
    "            for i, child in enumerate(family_tree[node]):\n",
    "                child_x = x + (i - (num_children - 1) / 3) * child_spacing\n",
    "                child_y = y - 1\n",
    "                ax.plot([x, child_x], [y, child_y], color='gray')\n",
    "                plot_tree(child, child_x, child_y, level + 1)\n",
    "    \n",
    "    plot_tree(\"Queen Elizabeth II\", 5, 5.5)\n",
    "    plt.title(\"Genealogical Tree Visualization\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b758c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Treemap example with the Budget Tree synthetic data\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Define data for the treemap\n",
    "data = {\n",
    "    'labels': ['Gross Salary','Expenses','Savings','House','Utils','Learn','Leisure','Market',\n",
    "               'Invest','Retire','Rental','Maint','Power','W&W','College','Others'],\n",
    "    'parents': ['', 'Gross Salary', 'Gross Salary', 'Expenses', 'Expenses', 'Expenses', \n",
    "                'Expenses', 'Expenses', 'Savings', 'Savings', 'House', 'House', \n",
    "                'Utils', 'Utils', 'Learn', 'Learn'],\n",
    "    'values': [6000, 5000, 1000, 1500, 400, 1600, 600, 900, 400, 600, 1200, 300, 250, \n",
    "               150, 1200, 400]\n",
    "}\n",
    "\n",
    "# Create a treemap\n",
    "fig = px.treemap(data,names='labels',parents='parents',values='values',branchvalues='total')\n",
    "\n",
    "# Update font size of labels\n",
    "fig.update_traces(textinfo='label+percent entry+value')  # Add labels parameter here\n",
    "fig.update_layout(title_x=.5)\n",
    "\n",
    "# Show the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunburst example with the Budget Tree synthetic data\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define data for the sunburst chart\n",
    "labels = ['Gross Salary','Expenses','Savings','House','Utils','Learn','Leisure','Market',\n",
    "         'Invest','Retire','Rental','Maint','Power','W&W','College','Others']\n",
    "parents = ['','Gross Salary','Gross Salary','Expenses','Expenses','Expenses','Expenses',\n",
    "           'Expenses','Savings','Savings','House','House','Utils','Utils','Learn','Learn']\n",
    "values = [6000, 5000, 1000, 1500, 400, 1600, 600, 900, 400, 600, 1200, 300, 250, 150, 1200, 400]\n",
    "\n",
    "# Create a sunburst chart\n",
    "fig = go.Figure(go.Sunburst(labels=labels,parents=parents,values=values,branchvalues=\"total\"))\n",
    "\n",
    "# Set the title\n",
    "fig.update_traces(textinfo='label+percent entry+value')\n",
    "fig.update_layout(title=\"Sunburst Chart Example\",title_x=.5)\n",
    "\n",
    "# Show the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e6831",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to calculate Descriptive Statistics of the \n",
    "# Zachary's Karate Club Social Network\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Load the Zachary's Karate Club dataset\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Network Data Statistics\n",
    "print(\"Is the graph a tree?\", nx.is_tree(G))\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "print(\"Is the graph directed?\", G.is_directed())\n",
    "print(\"Is the graph connected?\", nx.is_connected(G))\n",
    "print(\"Average clustering coefficient:\", nx.average_clustering(G))\n",
    "print(\"Average shortest path length:\", nx.average_shortest_path_length(G))\n",
    "print(\"Number of connected components:\", nx.number_connected_components(G))\n",
    "print(\"Density:\", nx.density(G))\n",
    "print(\"Maximum degree:\", max(dict(G.degree()).values()))\n",
    "print(\"Minimum degree:\", min(dict(G.degree()).values()))\n",
    "print(\"Average degree:\", sum(dict(G.degree()).values()) / G.number_of_nodes())\n",
    "print(\"Assortativity coefficient:\", nx.assortativity.degree_assortativity_coefficient(G))\n",
    "print(\"Degree centrality:\")\n",
    "for node, centrality in nx.degree_centrality(G).items():\n",
    "    print(f\"Node {node}: {centrality:.4f}\")\n",
    "print(\"Betweenness centrality:\")\n",
    "for node, centrality in nx.betweenness_centrality(G).items():\n",
    "    print(f\"Node {node}: {centrality:.4f}\")\n",
    "print(\"Closeness centrality:\")\n",
    "for node, centrality in nx.closeness_centrality(G).items():\n",
    "    print(f\"Node {node}: {centrality:.4f}\")\n",
    "print(\"Eigenvector centrality:\")\n",
    "for node, centrality in nx.eigenvector_centrality(G).items():\n",
    "    print(f\"Node {node}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc31576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Visualize the Zachary's Karate Club Social Network in different layouts\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Zachary's Karate Club dataset\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Plot using spring layout\n",
    "pos_spring = nx.spring_layout(G, seed=4)\n",
    "plt.figure(figsize=(10, 25))\n",
    "\n",
    "plt.subplot(411)\n",
    "nx.draw_networkx(G, pos_spring, node_color='lightblue', font_size=8, edge_color='gray')\n",
    "plt.title(\"Spring Layout\")\n",
    "\n",
    "# Plot using circular layout\n",
    "pos_circular = nx.circular_layout(G)\n",
    "plt.subplot(412)\n",
    "nx.draw_networkx(G, pos_circular, node_color='lightblue', font_size=8, edge_color='gray')\n",
    "plt.title(\"Circular Layout\")\n",
    "\n",
    "# Plot using shell Layout\n",
    "shell_layout = [list(range(0, 17)), list(range(17, 34))]\n",
    "pos_shell = nx.shell_layout(G, nlist=shell_layout)\n",
    "plt.subplot(413)\n",
    "nx.draw_networkx(G, pos_shell, node_color='lightblue', font_size=8, edge_color='gray')\n",
    "plt.title(\"Shell Layout\")\n",
    "\n",
    "# Plot using spring layout with custom positions\n",
    "pos_custom = nx.spring_layout(G, seed=4, iterations=200)\n",
    "custom_positions = {0: (0.0, 1.5), 1: (.5, 1.5), 2: (0.5, -1.5), 33: (0.0, -1.5)}\n",
    "pos_custom.update(custom_positions)\n",
    "plt.subplot(414)\n",
    "nx.draw_networkx(G, pos_custom, node_color='lightblue', font_size=8, edge_color='gray')\n",
    "plt.title(\"Custom Spring Layout\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the heatmap for the Zachary's data \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Zachary's Karate Club dataset\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Plot the social network\n",
    "pos = nx.spring_layout(G, seed=4)  # Positioning algorithm with fixed seed for reproducibility\n",
    "\n",
    "# Heatmap\n",
    "adjacency_matrix = nx.to_numpy_matrix(G)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(adjacency_matrix, cmap='YlGnBu', linewidths=0.5, annot=False)\n",
    "plt.title(\"Adjacency Matrix Heatmap\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
